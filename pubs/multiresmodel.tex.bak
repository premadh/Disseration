\chapter[Methods of Multiresolution Modelling]{Methods for Multiresolution Modelling}
\markboth{Methods for Multiresolution Modelling}{}
\label{ch:multiresmodel}

\begin{fquote}[English Proverb]The more the merrier.
\fqsource{14th--century poem Pearl, line 850} \end{fquote} 

The abundance of multiresolution data and increasing benefits of
analyzing multiple datasets within single analysis have given
major impetus to analysis of multiresolution data. In application
areas where division of data across different resolution is smooth,
wavelets~\cite{jawerth94}, multiscale 
methods~\cite{barth2002multiscale,e2011principles}, and 
scale--space theory~\cite{lindeberg94} have been popularly 
used to analyze multiresolution data. This chapter discusses 
the core of the thesis and includes most of the scientific 
contributions of this thesis. This thesis extensively uses 
pattern mining methods to complement and evaluate the 
multiresolution modelling algorithm. Furthermore, semantic 
data mining methods are used to analyze the multiresolution 
data in presence of other background knowledge 
in~\citepub{j2}. Hence, this chapter starts with a brief 
summary of pattern mining.

\section{Pattern Mining}
\label{s:patternmining}
Pattern mining is a popular branch of data mining thats aims to 
extract interesting, relevant, and meaningful patterns from the 
data~\cite{han2007,hand01}. Itemsets are  the columns in a 0--1 
dataset that have high concentration of 1s and are used as 
patterns in a 0--1 dataset~\cite{tatti08}. Frequent itemset mining 
is one of the first and most popular pattern mining algorithm. 
Let $\mathcal{I}_1,\mathcal{I}_2,\ldots,\mathcal{I}_n $ be the 
attributes (items) of a dataset, $\mathcal{D}$, and $\sigma$ be 
the given support. A frequent set or a frequent itemset can be 
defined as a set $\mathcal{F}$ of items of $\mathcal{D}$ such 
that at least a fraction of $\sigma$ of the rows of $\mathcal{D}$ 
have 1 in all columns of $\mathcal{F}$~\cite{agrawal1993,mannila1994}.

Anti--monotone property of frequent itemset indicates that if an 
itemset $\lbrace a,b,c \rbrace$ is frequent than all its subsets 
are also frequent~\cite{gallo2008}. Hence frequent itemsets 
generates a larger number of patterns making it difficult to report and 
interpret. Maximal frequent itemset ameliorates the problem of 
a larger number of patterns in frequent itemsets. An itemset 
is maximal frequent if none of its immediate supersets is 
frequent~\cite{burdick01}. We use maximal frequent itemset 
in~\citepub{c1} to compare and report patterns across different 
resolutions.

Most of these initial studies in pattern mining focuses on 
finding interesting patterns such as frequent itemsets from the 
large databases in an unsupervised setting. Nevertheless, 
association rules have been used in classification~\cite{jovanoski01,liu98}. 
Continuing with the research on association rules and classification, 
domain of subgroup discovery  has emerged as a popular data mining 
methodology for labeled data.  Subgroup discovery aims at finding 
interesting rules from the data that  best 
describes the target variable~\cite{gamberger02,franciso11,novak09}. 
Similarly, contrast set mining aims to learn the variables that 
differentiates one group of target variables from the rest, i.e., 
the most discriminating sets of variables~\cite{bay01,novak09}.


Recently, semantic data mining has recently gained research 
interest in pattern mining community because of the availability 
of large amount of data in the form ontologies encoded in semantic
web~\cite{lavrac2011}.  Especially, the additional knowledge are 
abundantly available in biology as discussed in 
Section~\ref{s:ontologymulti}. Semantic data mining method is a branch 
of data mining that uses taxonomies and ontologies of background data 
to improve the performance of  
algorithms~\cite{lavrac2011,AnzeCJ2012,Vavpetic13jiis}. In~\citepub{j2},
we use semantic data mining algorithm to explain the clustering 
results obtained by probabilistic clustering using background knowledge
discussed in Section~\ref{s:ontologymulti}.


\section{Data Transformation}
\label{s:dataTransformation}
Mixture models in their standard form are unable to model 
multiresolution data. Therefore, in~\citepub{c1}, we propose data 
transformation methods to model multiresolution data. The proposed 
methods transform data in different resolutions to a single resolution,
and combine them. We can then apply the modelling algorithm on the 
combined data in single resolution. The methodology of data transformation,
combination and modelling resembles multiresolution modelling
using data fusion techniques as in~\cite{carter1998analysis}.
Data transformation methods proposed in~\citepub{c1} are deterministic
and non--stochastic. Data transformation methods are also called sampling.
Sampling resolution 
in genomics explains the level of precision for measuring the results 
of the particular experiment: either global (coarse resolution) or 
detailed (fine resolution). As discussed in Section~\ref{s:multchrdata}
and also as shown in the Figure~\ref{Fig:chr21}, the relationship
between different resolutions of chromosome, i.e., correspondence of each 
of the regions in genome in different resolutions are known 
from~\cite{shaffer05}. We use two sampling methods to transform data 
across fine and coarse resolution based on the knowledge of 
correspondence of chromosomal regions in  different resolutions. 

\begin{enumerate}
\item Upsampling transforms the data from coarse resolution 
to fine resolution  increasing the dimensionality of the data.
We make multiple copies of a chromosomal region in coarser 
resolution to upsample the data to finer resolution. 
\item Downsampling transforms the data resolution from fine 
resolution to coarse resolution  decreasing the dimensionality 
of the data. We downsample using three different methods:
OR--function, Weighted, and Majority Decision. We consider the
chromosomal amplification pattern of neighboring chromosomal
regions if the number of  aberrated chromosomal regions and 
number of unaberrated  chromosomal regions are equal.
\begin{enumerate}
\item In OR--function downsampling, a chromosomal region in coarse
resolution is aberrated if any of the chromosomal regions in fine 
resolution is aberrated.
\item Division of the regions of a chromosome are highly irregular 
and the length of a region  often differs from the 
next~\cite{shaffer05}. In weighted downsampling, a chromosomal 
region in coarse resolution is aberrated if total length of the 
aberrated chromosomal regions is greater than total length of 
unaberrated chromosomal regions in fine resolution.
\item In majority decision downsampling, a chromosomal region in
coarse resolution is aberrated if majority chromosomal regions 
in fine resolution are aberrated.
\end{enumerate}
\end{enumerate}

\subsection{Experiments on Data Transformation}
\label{ss:DtaTransExp}

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{figures/pub1exp}
\caption[Experimental Procedure in~\citepub{c1}.]
{Experimental Procedure in~\citepub{c1} focuses 
on transformation of data across different resolutions and 
comparing them using different metrics.} 
\label{Fig:transexp}
\end{figure}
Figure~\ref{Fig:transexp} depicts the overall experimental procedure
where one of the three different downsampling methods
transforms the data to coarse resolution. Similarly, a deterministic
upsampling method transforms the data to fine resolution. Before
data transformation, data in original resolution is
also modeled using finite mixture model of multivariate Bernoulli
distributions. We integrate the data obtained in same resolution after
data transformation. The integrated data is again subjected to
finite mixture model of multivariate Bernoulli distributions. Finally, we 
compare the results of this analysis according to the patterns obtained and 
model fitting. We also compare three different downsampling methods using
metrics such as Frobenius norm.

Experimental results in~\citepub{c1} shows that the resulting data
produced by three downsampling methods are similar to each--other. 
The variation, if any are minute. Nevertheless, among the three 
methods the weighted downsampling method produces comparatively 
different result than majority decision and OR--function downsampling 
methods. Experiments with mixture modelling in different resolutions 
reported in~\citepub{c1} show that data likelihood of the 
mixture models is higher in coarser resolution compared to 
finer resolution. However, the model selection results 
is similar across different resolutions as similar number 
of components are selected.
Furthermore, time complexity is higher in the models in 
the finer resolution. This degradation of performance in 
fine resolution data can be attributed to ``curse of 
dimensionality'' phenomenon~\cite{bellman61,hughes68} or
Hughes effect~\cite{hughes68}.  Models in coarse resolution 
are also suitable for understanding and interpreting the  
results~\cite{Hollmen2007a}. 

The results in~\citepub{c1} also show that the algorithms 
produce better results on the combined data than the 
standalone data in single resolution. This proves the 
property of mixture model which states that 
number of components in the mixture model scales with the 
complexity of the data not with the sample size of the 
data~\cite{hammoud2000}. The increased sample size 
arising from the integration of data from other resolution 
helps nullify curse of dimensionality or the Hughes effect
and constrain the complexity of mixture models.


MAFIA (MAximal Frequent Itemset Algorithm)~\cite{burdick01} was
used to mine maximal frequent itemsets in data in the original 
resolution and the sampled resolution to determine if sampling 
preserves the patterns in the data. The results 
in~\citepub{c1} show that sampling of resolution preserves
the maximal frequent itemsets with negligible differences.
The negligible differences are expected because data in 
coarse resolution cannot subsume all the information of 
the data in fine resolution. In~\cite{adhikari2010b},
we have also performed statistical significance testing
of the Frequent Itemsets (not maximal frequent itemset) 
and show that sampling across different resolution preserves
the statistically significant patterns present in the data.
Furthermore, results in~\cite{adhikari2010b} also shows
that statistically significant patterns are also preserved
by the mixture models in all the resolutions.


\section{Merging of Mixture Components}
\label{s:meringMixCompo}
In~\citepub{c2}, we use merging of the mixture components of the mixture 
models in different resolution to model multiresolution data. 
Proposed multiresolution modeling algorithm resembles clustering
aggregation algorithm in~\cite{gionis07ca}. Here, we first model 
the data in each resolution separately using mixture models. Secondly, we 
iteratively merge the similar mixture components in different 
mixture models in different resolutions, unlike the components 
from the same mixture model as in~\citepub{j1}. We extend the 
derivation of fast approximation of Kullback Leibler  divergence 
as a criterion to determine the  similarity between the mixture  
components to two mixture models  from~\citepub{j1} as:

\begin{eqnarray}
\label{eq:symmcfmulti}
KL & = & \displaystyle \sum_{i \in X^{*}} \pi_{\alpha} \displaystyle
\prod _{m=1}^{{d}}
\left(\alpha_m^{X^{*}_{im}}(1-\alpha_{m})^{(1-X^{*}_{im})} \right) \\ \nonumber
& &- \displaystyle \sum_{i{\prime} \in Y^{*}} \pi_{\beta} \displaystyle \prod
_{n=1}^{{d^{\prime}}} \left(
\beta_{n}^{Y^{*}_{i{\prime}n}}(1-\beta_{n})^{(1-Y^{*}_{i{\prime}n})} \right)
\end{eqnarray}

The approximation of KL divergence in Equation~\ref{eq:symmcfmulti} 
resembles Equation~\ref{eq:symmcf} except that the two component 
distributions $\alpha$ and $\beta$ are components of two different
mixture models in two different resolutions.

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{figures/idasecondpage}
\caption[Similar mixture components in two resolutions.]
{Similarity of mixture components in two different resolutions.} 
\label{Fig:idasecondpage}
\end{figure}

We calculate the 
KL divergence among all the components in two mixture model and 
select the similar components using minimum weight bipartite 
matching algorithm~\cite{west96graph} as shown in Figure~\ref{Fig:idasecondpage}. 
We then merge the similar components preserving the properties of 
component distributions and probability of random values. 
We iterate the matching shown in Figure~\ref{Fig:idasecondpage} 
and merging of mixture components until the  KL divergence is small 
enough. Finally, we retrain the mixture model in each resolution 
producing the mixture model in different resolutions. Although 
mixture models are generated separately in each resolution, they
incorporates information in other resolutions.


\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{figures/nbarlkhood}
\caption[Similar mixture components in two resolutions.]
{Similarity of mixture components in two different resolutions.} 
\label{Fig:barlikelihood}
\end{figure}

The algorithm generates plausible results as reported in~\citepub{c2} when the algorithm is 
experimented on multiresolution chromosomal amplification datasets 
discussed in Section~\ref{s:multchrdata}. The bar diagram in the 
Figure~\ref{Fig:barlikelihood} depicts the training and validation 
likelihood of the multiresolution and independently trained single 
resolution mixture models trained in a 10--fold cross--validation 
setting. Since the units in Y--axis 
denotes negative log--likelihood, the shorter the bar better                    %%%%%%%%%%%%%%%%%%% TOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO DOOOOOOOOOOOOOO
the result. The Figure~\ref{Fig:barlikelihood} shows two different 
conditions of the likelihood: first, single resolution, and multiresolution 
model on the coarse data which is enclosed in the red rectangle in
the left. Second,  Figure~\ref{Fig:barlikelihood} also shows
results of single resolution and multiresolution models on the 
fine data which is enclosed in blue rectangle in the right.


Scrutinizing the results with in the both dotted and solid 
rectangles in the Figure~\ref{Fig:barlikelihood}, the 
performance of the multiresolution model is markedly better in 
the coarse resolution and slightly better in the fine resolution. 
The improvement in performance of multiresolution model in coarse 
resolution is greater than in the fine resolution because the 
number of data samples is comparatively smaller in the coarse 
resolution to add more information to the model in the fine 
resolution. As discussed in Section~\ref{s:meringMixCompo},
the performance of the models are better in coarse resolution
because of the curse of dimensionality or Hughes effect. We
also performed the two--tailed t--test to ascertain the 
statistical significance of the result on
the data likelihood~\cite{walpole2012}. 
The results show that both the validation and the training 
likelihoods are statistically significant when the 
significance level, $\sigma$, is 0.1. 


\section{Multiresolution Mixture Components from Domain Ontology}
\label{s:DomainOnto}

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{figures/bnnfig1}
\caption[Bayesian Network from Domain Knowledge.]
{Example of Part of Hierarchy in real world. Such part of hierarchy 
form the multiresolution data.} 
\label{Fig:oncobnn}
\end{figure}


Multiresolution data often forms hierarchical structure 
as discussed in Chapter~\ref{ch:analysismultires}.
We can exploit this structural information 
from the application area to determine the relationships 
between data resolutions. Consequently, as shown in the 
Figure~\ref{Fig:oncobnn}, we can determine the structure 
of the Bayesian network with some realistic assumptions 
such as the data features in the coarse resolution form 
the root and branches near the root of the Bayesian 
network. Similarly, the data 
features in the fine resolution form the branches 
towards the leaves and the leaves of the 
Bayesian network. Additionally, we can assume that the 
directed arrows originate from the features in the coarse 
resolution for computational efficiency. For example, 
right panel in the Figure~\ref{Fig:oncobnn} shows a Bayesian 
Network generated from the hierarchical structure of data 
depicted in the left panel. In real life, although the hierarchical
structure as shown in the left of the Figure~\ref{Fig:oncobnn}
known, but data in all the resolutions are unavailable. Bayesian 
networks have been widely used in application areas
such as medicine, computational biology and 
bioinformatics~\cite{friedman2000}. Bayesian networks have also 
been used to impute missing values~\cite{dizio2004}. Likewise 
in~\citepub{c3}, Bayesian networks in the component distributions 
are used to impute missing data resolutions. 
Experiments in~\citepub{c3} show that performance of 
Bayesian networks is satisfactory when imputing missing
data resolutions.

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{figures/multiresmix}
\caption[Multiresolution Mixture Model.]
{Structure of multiresolution mixture model whose components
are Bayesian networks.} 
\label{Fig:multiresmix}
\end{figure}

Figure~\ref{Fig:multiresmix} depicts the structure of the
proposed mixture model in~\citepub{c3} for data in multiple 
resolutions. The component distributions of the mixture 
model are Bayesian networks. The three solid rectangles
on the top represent different mixture coefficients, $\pi$. 
Similarly, the three network of nodes denote the three 
component distributions where each node defines a parameter of 
the component distribution, $\theta$. The structure of the component 
distribution comes from the domain knowledge. 
Therefore, the structure of Bayesian network is known and the 
parameters of these Bayesian networks can be learned 
using maximum likelihood framework~\cite{barberBRML12}.

The challenges concerning model selection in 
multiresolution mixture model culminates to 
determining the optimal number of such component 
distributions, i.e., Bayesian networks present in 
the data~\cite{fraley1998}. Similarly, learning 
the parameters of the component distributions 
involves learning the parameters of those networks. 
In the general framework for the EM algorithm, we 
can assign only a single probability value to a 
node in the mixture model~\cite{expectmax}. However, each 
variable in Bayesian network consists of minimum 
of two probability values denoting the CPD of the
nodes. Therefore, we learn the mixture  model in 
the two step procedure. First, we learn the 
individual Bayesian networks in the framework of 
Bayesian networks~\cite{barberBRML12,heckerman1999}.
Second, we transform the networks to vectors to
learn with IID assumption, the parameters of mixture 
model using the EM  algorithm as in~\cite{tikka2007b}.



\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{figures/barlkhood}
\caption[Performance comparison of mixture models.]
{Likelihood of single resolution and multiresolution 
mixture model on simulated dataset.} 
\label{Fig:barmultires}
\end{figure}
In addition to the multiresolution chromosomal amplification 
datasets discussed in Section~\ref{s:multchrdata}, we 
have in~\citepub{c3} experimented with a simulated 
dataset that allows observation of complete data
without missing resolutions. 
The bar diagram in the Figure~\ref{Fig:barmultires} 
displays the likelihood of the a multiresolution 
mixture model trained in a 10--fold cross--validation 
setting and also three different single resolution 
mixture models trained individually in each resolution. 
Since units used in the Y--axis is negative loglikelihood, 
the shorter the bar better the result. The 
Figure~\ref{Fig:barmultires} shows two different conditions 
of likelihood: training  and validation. However,  
the reported results does not depict change in training 
and validation likelihood during model selection, instead
it shows the difference in training and validation 
likelihoods after the selection of components. 

The Figure~\ref{Fig:barmultires} shows that the 
performance of the multiresolution model  is 
markedly better than the three single resolution
models. Log likelihood is comparatively smaller 
in dimensionality of fifteen, and twentyfive because of the 
larger data dimensionality demonstrating curse of
dimensionality or Hughes phenomena. The likelihood
of the proposed multiresolution model is better 
than the data with the smallest dimensionality of five in 
single resolution showing that proposed multiresolution
mixture model produces plausible results in addition
to its facilities of providing single analysis for 
the data in multiple resolutions.


\section{Semantic Data Mining in Multiresolution Modeling}
\label{s:Semantic}

As discussed in Section~\ref{s:ontologymulti} semantic data mining 
methods have been gaining popularity in the data mining domain. 
Similarly, matrix methods such as banded matrices have also 
found profound usage in data mining domain~\cite{elden2007,garriga2011banded}.
In~\citepub{j2}, we provide comprehensive analysis of 
multiresolution data using a three stage methodology 
depicted in Figure~\ref{Fig:sematicworkflow}.
In~\citepub{j2}, we explain the clustering generated by mixture
models using semantic data mining methods and visualize the 
clusters and the semantic rules using banded matrices. 


\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{figures/sematicworkflow}
\caption[Workflow of Semantic Data Mining of Cluster.]
{The workflow for comprehensive analysis of multiresolution
data using a combination of probabilistic model based clustering, 
semantic data mining, and banded matrices.} 
\label{Fig:sematicworkflow}
\end{figure}

Figure~\ref{Fig:sematicworkflow} shows that input to the 
methodology is the relevant data and additional background
knowledge. The data is the chromosomal amplification data
discussed in Section~\ref{s:multchrdata}. Similarly, 
%input to the methodology is 
the additional background 
knowledge is used by the semantic data mining algorithm
to supplement the analysis of the empirical data. 
As cancer is a heterogeneous and multifactorial 
disease~\cite{king2002genetic}, we use additional 
background knowledge with an aim to facilitate the 
understanding and interpretability of the results. 
%The additional background knowledge
%aid the data mining algorithm to garner information from
%the data. 
The additional knowledge provided to the semantic
data mining algorithm comprises of fragile 
sites~\cite{durkin07,schwartz06}, 
cancer genes~\cite{futreal2004},  
amplification hotspots~\cite{myllykangas06}, 
and virus integration sites~\cite{khoury2013,zurHausen2009}.
Finally, the taxonomies of hierarchical regions of chromosomes
discussed in Section~\ref{s:multchrdata}
are also used as additional background knowledge

\begin{figure}[h!]
\centering
\includegraphics[width=0.9\textwidth]{figures/clusterrules}
\caption[Visualization of Clusters and Rules through Banded Matrices.]
{The comprehensive analysis of multiresolution data using
a combination of probabilistic model based clustering, 
semantic pattern mining, and banded matrices.} 
\label{Fig:clusterrules}
\end{figure}

Mixture models provide an ability to cluster the data
considering the components in the mixture model as 
a cluster~\cite{mclachlan1988mixture,melnykov2010finite}.
We model the data training the models in a ten--fold cross--validation 
setting and selecting the components that generalizes best on
the left--out data samples taking parsimony into account~\cite{tikka2007b}.
The results produced by mixture models are complex to 
explain to the application area specialist. Although efforts 
have been made to make the results understandable to the 
domain experts~\cite{Hollmen2007a}. In~\citepub{j2}, we make 
an effort to explain those clusters according to the rules generated
by semantic data mining algorithms and banded matrices
visualizations. The cluster labels generated by mixture model 
clustering are used as class labels in semantic pattern mining
algorithm along with the additional background knowledge. 
In~\citepub{j2}, we use general purpose semantic subgroup 
discovery system, Hedwig to find a hypothesis (a predictive 
model or a set of descriptive patterns) in domain ontology
terms given the training data and the domain knowledge in the 
form of ontologies~\cite{Vavpetic13jiis}. 

\begin{table}[bt]
\centering
\begin{tabular}{clccc}\hline
\textbf{\#} & \textbf{Rules for cluster 3} & \textbf{TP} & \textbf{FP} & \textbf{Precision}\\\hline
1 & \texttt{Cluster3(X) $\leftarrow$ 1q43-44(X) $\wedge$ 1q12(X)} & 81 & 0 & 1.00  \\
2 & \texttt{Cluster3(X) $\leftarrow$ 1q11(X)} & 78 & 9 & 0.90  \\
3 & \texttt{Cluster3(X) $\leftarrow$ 1q43-44(X)} & 88 & 26 & 0.77  \\
4 & \texttt{Cluster3(X) $\leftarrow$ 1q41(X)} & 88 & 28 & 0.76  \\
5 & \texttt{Cluster3(X) $\leftarrow$ 1q12(X)} & 81 & 43 & 0.65  \\
6 & \texttt{Cluster3(X) $\leftarrow$ 1q32(X)} & 88 & 52 & 0.63  \\
7 & \texttt{Cluster3(X) $\leftarrow$ 1q31(X)} & 87 & 54 & 0.62  \\
8 & \texttt{Cluster3(X) $\leftarrow$ 1q25(X)} & 88 & 64 & 0.58  \\
9 & \texttt{Cluster3(X) $\leftarrow$ 1q24(X)} & 88 & 97 & 0.48  \\
10 & \texttt{Cluster3(X) $\leftarrow$ 1q21(X)} & 88 & 134 & 0.40  \\
11 & \texttt{Cluster3(X) $\leftarrow$ 1q22--24(X)} & 88 & 149 & 0.37  \\
12 & \texttt{Cluster3(X) $\leftarrow$ HotspotSite(X)} & 88 & 222 & 0.28 \\
13 & \texttt{Cluster3(X) $\leftarrow$ CancerSite(X)} & 88 & 245 & 0.26 \\
14 & \texttt{Cluster3(X) $\leftarrow$ FragileSite(X)} & 88 & 259 & 0.25 \\
\hline
\end{tabular}
\caption{Rules induced for 3 using semantic data mining algorithm Hedwig.}
\label{Tab:clus3rules}
\end{table}


We use the constrained banded matrices~\cite{garriga2011banded} to visualize the data
because in chromosomal amplification data the columns denotes the 
specific and unchangeable chromosome regions. We therefore shuffle
only the rows, i.e., samples and not the columns. We overlay cluster
information along the rows of banded matrix which shows clear 
distinction among different clusters. We overlay
the cluster information along the rows as shown in the left panel 
of the Figure~\ref{Fig:clusterrules}. 


In addition, we overlay 
the rules generated using the semantic subgroup discovery method a
s shown in the right panel of the Figure~\ref{Fig:clusterrules}. 
The visualized rules are tabulated in Table~\ref{Tab:clus3rules}.
The numbers above the rules on top right corner denote the position 
of rules in the table. A darker hue means that specific region
in chromosome appears in more than one rule.
Overlaying all the clusters and the rules for each of the clusters 
will clutter multitude information on a single figure making 
it difficult to understand. 
Therefore, we first visualize all the clusters in the data 
overlaying it on a banded matrix as shown in the left panel of 
the Figure~\ref{Fig:clusterrules}. Second, we visualize only a 
single cluster and the rules describing that cluster as shown
in the right panel of the Figure~\ref{Fig:clusterrules}.



The left panel of the Figure~\ref{Fig:clusterrules} distinctly 
shows different clusters proving the credibility of
the clustering results. Similarly, the rules visualized in 
the right panel of the Figure~\ref{Fig:clusterrules} identify
specific amplifications in chromosomal regions that are 
responsible for certain cluster and consequently, specific 
groups of cancer as shown in~\cite{myllykangas08}. Furthermore,
the rules generated by semantic data mining algorithm provides
additional insights into the clustering solutions. For example,
from the left panel of the Figure~\ref{Fig:clusterrules} cluster
3 is denoted by the pronounced amplification in regions 1q11-q44. 
The rule: \framebox[1.1\width]{Rule 1: \texttt{Cluster3(X) 
$\leftarrow$ 1q43--44(X) $\wedge$ 1q12(X)}} characterizes 
81 out of 88 data samples that are in cluster 3 showing that
amplifications in regions  1q43--44 and 1q12 characterizes 
cluster 3 and related cancers with good coverage and precision. 
It is not necessary for the whole  region of 1q11--44 to be aberrated 
to discriminate that specific cluster of cancers.




%We are interested in 
%modeling 0-1 data so we consider the mixture model of multivariate
%bernoulli distributions. The standard formulation of finite mixture 
%models of bernoulli distributions is shown in the equation. We are 
%interested in modelling a probability distribution P(x) in terms of 
%J-component distributions parameterized by theta .. js. We then mix
%them together to form the probability distribution. Since our data 
%is 0-1 data, the probability distribution is multivariate bernoulli 
%as shown on the right side of the equation. 'J' indexes the components 
%and 'I' indexes the data dimension. These parameters, theta, tell the 
%probability that a random variable takes the value 0 or 1. Estimation 
%of the model can be performed easily using EM algorithm. This is a 
%text book stuff and have been there since 1970s so I will not touch 
%this. We have an open-source program package to learn mixture models 
%of multivariate bernoulli distributions. 
%components each resolution.
%gives additional 
%practical usefulness to mixture models. Among many 
%different uses of Bayesian networks, we 
%The figure 
%depicts a mixture model having the Bayesian networks as the 
%components for data in multiple resolutions. 
%\section{Section Heading}

%although the dimensionality 
%of the multi-resolution data is 9. 

%The Figure~\ref{Fig:barmultires} does not show the difference
%between training and validation likelihood u


%There is growing interest
%in multiresolution modelling because of the benefits of combining
%multiple data


%Recent addition to the field of pattern mining is rule based learning, 
%often referred to as association rule is a popular data mining  
%methodology to determine the interesting relations between variables 
%based on different various measures of 
%interestingness~\cite{hipp00,klemettinen94,piatetsky91b,agrawal94}. 

%Additionally, algorithms for determining 
%emerging patterns which are itemsets that significantly differs from one class 
%to the next~\cite{novak09,dong99}. 
